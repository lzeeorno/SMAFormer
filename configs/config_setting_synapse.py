from pickle import FALSE
from torchvision import transforms
from datasets.dataset import *
from utils import *

from datetime import datetime
import ml_collections

class setting_config:
    """
    the config of training setting.
    """
    # æ¨¡å‹é€‰æ‹©é…ç½® - æ–°å¢åŠŸèƒ½
    available_models = ['DWSegNet', 'AFFSegNet', 'vmunet', 'HBFormer', 'SMAFormer', 'SMAFormerV2']  # å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
    network = 'SMAFormerV2'  # åˆ‡æ¢åˆ°SMAFormerè¿›è¡Œè®­ç»ƒ
    
    affsegnet_config = {
        'num_classes': 9, 
        'input_channels': 3, 
        'feature_size': 48,  # AFFSegNetçš„ç‰¹å¾ç»´åº¦
        'use_boundary_refinement': False,
        # å¯é€‰çš„é¢„è®­ç»ƒæƒé‡è·¯å¾„
        'load_ckpt_path': '',  # æš‚æ—¶ä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
        # vmunetç‰¹å®šé…ç½®
        'depths': [2, 2, 9, 2],
        'depths_decoder': [2, 9, 2, 2],
        'drop_path_rate': 0.2,
    }

    dwsegnet_config = {
        'num_classes': 9, 
        'input_channels': 3, 
        'feature_size': 48,  # AFFSegNetçš„ç‰¹å¾ç»´åº¦
        'use_boundary_refinement': False,
        # å¯é€‰çš„é¢„è®­ç»ƒæƒé‡è·¯å¾„
        'load_ckpt_path': '',  # æš‚æ—¶ä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
        # vmunetç‰¹å®šé…ç½®
        'depths': [2, 2, 9, 2],
        'depths_decoder': [2, 9, 2, 2],
        'drop_path_rate': 0.2,
    }

    hbformer_config = {
        'num_classes': 9,
        'input_channels': 3,
        'feature_size': 48,
        'use_boundary_refinement': False,
    }

    # SMAFormer é…ç½® - IEEE BIBM 2024 (æ”¹è¿›ç‰ˆæœ¬ - é›†æˆæ–¹æ¡ˆA/B/C/D)
    # âš ï¸ é‡è¦æç¤º: æµ‹è¯•æ—¶é…ç½®å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´!
    # å¦‚æœè®­ç»ƒæƒé‡æ˜¯ç”¨baselineè®­ç»ƒçš„ï¼Œè¯·è®¾ç½®: sma_mode='disabled', use_multi_scale=False, use_enhanced_decoder=False
    # å¦‚æœè®­ç»ƒæƒé‡æ˜¯ç”¨å®Œæ•´ç‰ˆè®­ç»ƒçš„ï¼Œè¯·ä¿æŒ: sma_mode='parallel', use_multi_scale=True, use_enhanced_decoder=True
    smaformer_config = {
        'num_classes': 9,
        'input_channels': 3,
        'embed_dim': 768,  # ViT-Baseå›ºå®š768
        'depth': 12,  # ViT-Baseå›ºå®š12ä¸ªblocks
        'num_heads': 12,  # ViT-Baseå›ºå®š12ä¸ªheads
        'mlp_ratio': 4.,
        'drop_rate': 0.,
        'attn_drop_rate': 0.,
        'drop_path_rate': 0.1,
        'pretrained': True,
        'pretrained_model': 'vit_base_patch16_224',  # ä½¿ç”¨ViT-Base
        'pretrained_path': 'pre_trained_weights',
        
        # ========== æ”¹è¿›æ–¹æ¡ˆå¼€å…³ (è®­ç»ƒå’Œæµ‹è¯•è‡ªåŠ¨åŒæ­¥æ­¤é…ç½®!) ==========
        # ğŸ’¡ åªéœ€ä¿®æ”¹è¿™é‡Œï¼Œtrainå’Œtestéƒ½ä¼šè‡ªåŠ¨ä½¿ç”¨ç›¸åŒé…ç½®
        # æ–¹æ¡ˆA: SMAä½ç½®é‡æ„
        'sma_mode': 'parallel',  # 'parallel'=å¹¶è¡ŒSMA+é—¨æ§(æ–¹æ¡ˆA), 'disabled'=ç¦ç”¨SMA(baseline), 'original'=åŸå§‹ä¸²è¡ŒSMA
        
        # æ–¹æ¡ˆB: å¤šå°ºåº¦ç‰¹å¾æå–ï¼ˆDPTé£æ ¼ï¼‰
        'use_multi_scale': True,  # True=å¼€å¯å¤šå°ºåº¦ç‰¹å¾(æ–¹æ¡ˆB), False=ä¸ä½¿ç”¨å¤šå°ºåº¦
        
        # æ–¹æ¡ˆC: å¢å¼ºDecoderï¼ˆASPP + SE + Skip Connectionsï¼‰
        'use_enhanced_decoder': True,  # True=å¼€å¯å¢å¼ºDecoder(æ–¹æ¡ˆC), False=ä½¿ç”¨åŸºç¡€Decoder
        
        # é…ç½®ç¤ºä¾‹:
        # 1. å®Œæ•´ç‰ˆ (æ¨è): sma_mode='parallel', use_multi_scale=True, use_enhanced_decoder=True  (138.89Må‚æ•°)
        # 2. ä»…å¹¶è¡ŒSMA: sma_mode='parallel', use_multi_scale=False, use_enhanced_decoder=False  (126.12Må‚æ•°)
        # 3. ä»…å¤šå°ºåº¦: sma_mode='disabled', use_multi_scale=True, use_enhanced_decoder=False    (97.51Må‚æ•°)
        # 4. Baseline: sma_mode='disabled', use_multi_scale=False, use_enhanced_decoder=False    (89.77Må‚æ•°)
        
        # å‘åå…¼å®¹
        'use_sma': True,  # ä¿ç•™æ—§å‚æ•°ï¼ˆå·²åºŸå¼ƒï¼Œç”±sma_modeæ§åˆ¶ï¼‰
    }

    # SMAFormerV2 é…ç½® - åŸºäºHBFormeræ¶æ„çš„å¢å¼ºç‰ˆ
    # ä½¿ç”¨Swin Transformer backbone + SMA enhancement
    smaformerv2_config = {
        'num_classes': 9,
        'input_channels': 3,
        'feature_size': 48,  # åŸºç¡€ç‰¹å¾ç»´åº¦
        'img_size': (256, 256),
        'swin_pretrained_path': 'pre_trained_weights/swin_tiny_patch4_window7_224.pth',
        'use_sma_in_encoder': True,  # æ˜¯å¦åœ¨encoderä¸­ä½¿ç”¨SMAå¢å¼º
        'use_edge_enhancement': True,  # æ˜¯å¦ä½¿ç”¨è¾¹ç¼˜å¢å¼º
    }

    # VMUNet é…ç½® - æ¢å¤åŸç‰ˆè®¾ç½®
    vmunet_config = {
        'num_classes': 9, 
        'input_channels': 3,
        'model_name': 'vmunet_s',
        'depths': [2, 2, 2, 2],  # æ¢å¤åŸç‰ˆVMUNetå±‚æ·±åº¦é…ç½®
        'depths_decoder': [2, 2, 2, 1],  # æ¢å¤åŸç‰ˆVMUNetè§£ç å™¨å±‚æ·±åº¦é…ç½®
        'drop_path_rate': 0.2,  # VMUNetéœ€è¦çš„drop path rate
        'load_ckpt_path': None,
        'pretrained_path': 'pre_trained_weights/vmamba_small_e238_ema.pth',  # VMUNeté¢„è®­ç»ƒæƒé‡
        'load_pretrained': True,  # å¯ç”¨é¢„è®­ç»ƒæƒé‡åŠ è½½
    }

     # æ ¹æ®é€‰æ‹©çš„ç½‘ç»œè®¾ç½®model_config
    if network == 'vmunet':
        model_config = vmunet_config
    elif network == 'HBFormer':
        model_config = hbformer_config
    elif network == 'DWSegNet':
        model_config = dwsegnet_config
    elif network == 'AFFSegNet':
        model_config = affsegnet_config
    elif network == 'SMAFormer':
        model_config = smaformer_config
    elif network == 'SMAFormerV2':
        model_config = smaformerv2_config
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç½‘ç»œç±»å‹: {network}")



    datasets_name = 'Synapse'  # ä¿æŒæ•°æ®é›†åç§°ä¸ºSynapseï¼Œä½†ä¼šè½¬æ¢ä¸ºå¤§å†™
    # input_size_h = 224
    # input_size_w = 224
    input_size_h = 256
    input_size_w = 256
    if datasets_name == 'synapse' or datasets_name == 'Synapse':
        data_path = './data/Synapse/train_npz/'
        datasets = Synapse_dataset
        list_dir = './data/Synapse/lists/lists_Synapse/'
        volume_path = './data/Synapse/test_vol_h5/'
        # æ–°å¢ï¼šæ”¯æŒslice-by-sliceçš„æµ‹è¯•
        test_slice_path = './data/Synapse/train_npz/'  # æµ‹è¯•sliceä¹Ÿä½¿ç”¨è®­ç»ƒæ•°æ®è·¯å¾„
        test_list_file = 'test_slice.txt'  # ä½¿ç”¨sliceçº§åˆ«çš„æµ‹è¯•åˆ—è¡¨
    else:
        raise Exception('datasets in not right!')
    
    pretrained_path = '' # if using pretrained, please enter the path of weights
    num_classes = 9
    # å¼ºåŒ–Diceå’ŒFocalåœ¨æ€»æŸå¤±ä¸­çš„æƒé‡ï¼Œä»¥æ›´å…³æ³¨å°å™¨å®˜
    loss_weight = [0.2, 0.4, 0.4]  # CE, Dice, Focal weights
    criterion = CeDiceFocalLoss(num_classes, loss_weight)
    z_spacing = 1
    input_channels = 3

    distributed = False
    local_rank = -1
    num_workers = 1 #16
    seed = 2050
    world_size = None
    rank = None
    amp = False

    batch_size = 28  # batch size (28for HBFormer, 48for SMAFormer)
    epochs = 300  # æš‚æ—¶è®¾ç½®ä¸º5ä¸ªepochæµ‹è¯•ä¿®å¤æ•ˆæœ
    resume_training = False  # æ˜¯å¦æ¢å¤è®­ç»ƒï¼ŒFalseè¡¨ç¤ºé‡æ–°å¼€å§‹è®­ç»ƒ
    work_dir = 'results/' + network + '_' + datasets_name 
    # 'D:/CODES/MedSeg/BIBM22/results/datrm2_isic18_Sunday_04_September_2022_12h_04m_10s/'
    print_interval = 20  # æ›´é¢‘ç¹çš„è¾“å‡º
    val_interval = 2   # æ¯2ä¸ªepochéªŒè¯ä¸€æ¬¡
    # æ–°å¢ä¿å­˜é—´éš”é…ç½®
    save_interval = 50  # æ¯10ä¸ªepochä¿å­˜ä¸€æ¬¡æƒé‡

    test_weights_path = './results/SMAFormerV2_Synapse/checkpoints/best.pth'  # ä½¿ç”¨AFFSegNetçš„300epochæƒé‡è·¯å¾„

    threshold = 0.5

    opt = 'AdamW'  # Changed to AdamW as requested
    assert opt in ['Adadelta', 'Adagrad', 'Adam', 'AdamW', 'Adamax', 'ASGD', 'RMSprop', 'Rprop', 'SGD'], 'Unsupported optimizer!'
    if opt == 'Adadelta':
        lr = 0.01 # default: 1.0 â€“ coefficient that scale delta before it is applied to the parameters
        rho = 0.9 # default: 0.9 â€“ coefficient used for computing a running average of squared gradients
        eps = 1e-6 # default: 1e-6 â€“ term added to the denominator to improve numerical stability 
        weight_decay = 0.05 # default: 0 â€“ weight decay (L2 penalty) 
    elif opt == 'Adagrad':
        lr = 0.01 # default: 0.01 â€“ learning rate
        lr_decay = 0 # default: 0 â€“ learning rate decay
        eps = 1e-10 # default: 1e-10 â€“ term added to the denominator to improve numerical stability
        weight_decay = 0.05 # default: 0 â€“ weight decay (L2 penalty)
    elif opt == 'Adam':
        lr = 0.0001 # default: 1e-3 â€“ learning rate
        betas = (0.9, 0.999) # default: (0.9, 0.999) â€“ coefficients used for computing running averages of gradient and its square
        eps = 1e-8 # default: 1e-8 â€“ term added to the denominator to improve numerical stability 
        weight_decay = 0.05 # default: 0 â€“ weight decay (L2 penalty) 
        amsgrad = False # default: False â€“ whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond
    elif opt == 'AdamW':
        lr = 3e-4 # é™ä½å­¦ä¹ ç‡é¿å…æ¢¯åº¦çˆ†ç‚¸
        betas = (0.9, 0.999) # default: (0.9, 0.999) â€“ coefficients used for computing running averages of gradient and its square
        eps = 1e-8 # default: 1e-8 â€“ term added to the denominator to improve numerical stability
        weight_decay = 1e-3 # default: 1e-2 â€“ weight decay coefficient
        amsgrad = False # default: False â€“ whether to use the AMSGrad variant of this algorithm from the paper On the Convergence of Adam and Beyond 
    elif opt == 'Adamax':
        lr = 2e-3 # default: 2e-3 â€“ learning rate
        betas = (0.9, 0.999) # default: (0.9, 0.999) â€“ coefficients used for computing running averages of gradient and its square
        eps = 1e-8 # default: 1e-8 â€“ term added to the denominator to improve numerical stability
        weight_decay = 0 # default: 0 â€“ weight decay (L2 penalty) 
    elif opt == 'ASGD':
        lr = 0.01 # default: 1e-2 â€“ learning rate 
        lambd = 1e-4 # default: 1e-4 â€“ decay term
        alpha = 0.75 # default: 0.75 â€“ power for eta update
        t0 = 1e6 # default: 1e6 â€“ point at which to start averaging
        weight_decay = 0 # default: 0 â€“ weight decay
    elif opt == 'RMSprop':
        lr = 1e-2 # default: 1e-2 â€“ learning rate
        momentum = 0 # default: 0 â€“ momentum factor
        alpha = 0.99 # default: 0.99 â€“ smoothing constant
        eps = 1e-8 # default: 1e-8 â€“ term added to the denominator to improve numerical stability
        centered = False # default: False â€“ if True, compute the centered RMSProp, the gradient is normalized by an estimation of its variance
        weight_decay = 0 # default: 0 â€“ weight decay (L2 penalty)
    elif opt == 'Rprop':
        lr = 1e-2 # default: 1e-2 â€“ learning rate
        etas = (0.5, 1.2) # default: (0.5, 1.2) â€“ pair of (etaminus, etaplis), that are multiplicative increase and decrease factors
        step_sizes = (1e-6, 50) # default: (1e-6, 50) â€“ a pair of minimal and maximal allowed step sizes 
    elif opt == 'SGD':
        lr = 0.003 # â€“ learning rate
        momentum = 0.98 # default: 0 â€“ momentum factor
        weight_decay = 0.0001 # default: 0 â€“ weight decay (L2 penalty) 
        dampening = 0 # default: 0 â€“ dampening for momentum
        nesterov = False # default: False â€“ enables Nesterov momentum 
    
    sch = 'CosineAnnealingLR'  # Changed to CosineAnnealingLR as requested
    if sch == 'StepLR':
        step_size = epochs // 5 # â€“ Period of learning rate decay.
        gamma = 0.5 # â€“ Multiplicative factor of learning rate decay. Default: 0.1
        last_epoch = -1 # â€“ The index of last epoch. Default: -1.
    elif sch == 'MultiStepLR':
        milestones = [60, 120, 150] # â€“ List of epoch indices. Must be increasing.
        gamma = 0.1 # â€“ Multiplicative factor of learning rate decay. Default: 0.1.
        last_epoch = -1 # â€“ The index of last epoch. Default: -1.
    elif sch == 'ExponentialLR':
        gamma = 0.99 #  â€“ Multiplicative factor of learning rate decay.
        last_epoch = -1 # â€“ The index of last epoch. Default: -1.
    elif sch == 'CosineAnnealingLR':
        T_max = 300 # Set to 300 as requested
        eta_min = 1e-6 # â€“ Minimum learning rate. Default: 0.
        last_epoch = -1 # â€“ The index of last epoch. Default: -1.  
    elif sch == 'ReduceLROnPlateau':
        mode = 'min' # â€“ One of min, max. In min mode, lr will be reduced when the quantity monitored has stopped decreasing; in max mode it will be reduced when the quantity monitored has stopped increasing. Default: 'min'.
        factor = 0.1 # â€“ Factor by which the learning rate will be reduced. new_lr = lr * factor. Default: 0.1.
        patience = 10 # â€“ Number of epochs with no improvement after which learning rate will be reduced. For example, if patience = 2, then we will ignore the first 2 epochs with no improvement, and will only decrease the LR after the 3rd epoch if the loss still hasn't improved then. Default: 10.
        threshold = 0.0001 # â€“ Threshold for measuring the new optimum, to only focus on significant changes. Default: 1e-4.
        threshold_mode = 'rel' # â€“ One of rel, abs. In rel mode, dynamic_threshold = best * ( 1 + threshold ) in 'max' mode or best * ( 1 - threshold ) in min mode. In abs mode, dynamic_threshold = best + threshold in max mode or best - threshold in min mode. Default: 'rel'.
        cooldown = 0 # â€“ Number of epochs to wait before resuming normal operation after lr has been reduced. Default: 0.
        min_lr = 0 # â€“ A scalar or a list of scalars. A lower bound on the learning rate of all param groups or each group respectively. Default: 0.
        eps = 1e-08 # â€“ Minimal decay applied to lr. If the difference between new and old lr is smaller than eps, the update is ignored. Default: 1e-8.
    elif sch == 'CosineAnnealingWarmRestarts':
        T_0 = 50 # â€“ Number of iterations for the first restart.
        T_mult = 2 # â€“ A factor increases T_{i} after a restart. Default: 1.
        eta_min = 1e-6 # â€“ Minimum learning rate. Default: 0.
        last_epoch = -1 # â€“ The index of last epoch. Default: -1. 
    elif sch == 'WP_MultiStepLR':
        warm_up_epochs = 10
        gamma = 0.1
        milestones = [125, 225]
    elif sch == 'WP_CosineLR':
        warm_up_epochs = 20

    
    
    # 3Dé¢„æµ‹ç›¸å…³é…ç½®
    pred_3d_config = {
        'img_size': (256, 256),  # æ¨¡å‹è¾“å…¥å°ºå¯¸
        'output_size': [256, 256],  # æµ‹è¯•æ—¶çš„è¾“å‡ºå°ºå¯¸
        'device': 'cuda',  # ä½¿ç”¨çš„è®¾å¤‡ï¼Œå¯é€‰ 'cuda' æˆ– 'cpu'
        'use_auto_device': True,  # æ˜¯å¦è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
    }

    @classmethod
    def set_model(cls, model_name):
        """è®¾ç½®ä½¿ç”¨çš„æ¨¡å‹"""
        if model_name not in cls.available_models:
            raise ValueError(f"æ¨¡å‹å¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€: {cls.available_models}")
        cls.network = model_name
        print(f"è®¾ç½®æ¨¡å‹ä¸º: {model_name}")
        
        # æ›´æ–°å·¥ä½œç›®å½•
        if hasattr(cls, 'current_fold') and cls.current_fold is not None:
            cls.work_dir = f'results/{cls.network}_{cls.datasets_name}_fold{cls.current_fold}'
        else:
            cls.work_dir = f'results/{cls.network}_{cls.datasets_name}'
    
    @classmethod
    def set_save_interval(cls, interval):
        """è®¾ç½®ä¿å­˜é—´éš”"""
        if interval <= 0:
            raise ValueError("ä¿å­˜é—´éš”å¿…é¡»å¤§äº0")
        cls.save_interval = interval
        print(f"è®¾ç½®ä¿å­˜é—´éš”ä¸ºæ¯{interval}ä¸ªepochä¿å­˜ä¸€æ¬¡")
