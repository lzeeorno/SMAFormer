import torch
from torch import nn
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torchvision import transforms

from datasets.dataset import RandomGenerator
from engine_synapse import *

from models.DWSegNet import DWSegNet
from models.HBFormer import HBFormer
from models.vmunet.vmunet import VMUNet
from models.SMAFormer import SMAFormer
from models.SMAFormerV2 import SMAFormerV2

import os
import sys
import csv
import pandas as pd
import numpy as np
import torch.nn.functional as F
import random

os.environ["CUDA_VISIBLE_DEVICES"] = "0" # "0, 1, 2, 3"

from utils import *
from configs.config_setting_synapse import setting_config

import warnings
warnings.filterwarnings("ignore")


def seed_worker(worker_id):
    """è®¾ç½®DataLoader workerçš„éšæœºç§å­"""
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)


def main(config):

    print('#----------Creating logger----------#')
    sys.path.append(config.work_dir + '/')
    log_dir = os.path.join(config.work_dir, 'log')
    checkpoint_dir = os.path.join(config.work_dir, 'checkpoints')
    resume_model = os.path.join(checkpoint_dir, 'latest.pth')
    outputs = os.path.join(config.work_dir, 'outputs')
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)
    if not os.path.exists(outputs):
        os.makedirs(outputs)

    # Create CSV files for recording training and validation metrics
    train_csv_file = os.path.join(config.work_dir, 'train_record.csv')
    val_csv_file = os.path.join(config.work_dir, 'val_record.csv')
    
    # Define organ names for Synapse dataset (8 organs + background)
    organ_names = ['spleen', 'right_kidney', 'left_kidney', 'gallbladder', 
                   'esophagus', 'liver', 'stomach', 'aorta']
    
    # Training CSV columns
    train_columns = ['epoch', 'lr', 'loss', 'avg_dice', 'miou'] + [f'dice_{organ}' for organ in organ_names]
    
    # Validation CSV columns  
    val_columns = ['epoch', 'avg_dice', 'avg_hd95', 'miou'] + [f'dice_{organ}' for organ in organ_names] + [f'hd95_{organ}' for organ in organ_names]
    
    # å¦‚æœä¸æ˜¯æ¢å¤è®­ç»ƒï¼Œæ¸…ç©ºå¹¶é‡æ–°åˆ›å»ºCSVæ–‡ä»¶
    if not config.resume_training:
        # åˆ›å»ºæ–°çš„CSVæ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
        with open(train_csv_file, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=train_columns)
            writer.writeheader()
            
        with open(val_csv_file, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=val_columns)
            writer.writeheader()
    else:
        # æ¢å¤è®­ç»ƒæ—¶ï¼Œå¦‚æœCSVæ–‡ä»¶ä¸å­˜åœ¨åˆ™åˆ›å»º
        if not os.path.exists(train_csv_file):
            with open(train_csv_file, 'w', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=train_columns)
                writer.writeheader()
                
    if not os.path.exists(val_csv_file):
        with open(val_csv_file, 'w', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=val_columns)
            writer.writeheader()

    global logger
    logger = get_logger('train', log_dir)

    log_config_info(config, logger)





    print('#----------GPU init----------#')
    set_seed(config.seed)
    gpu_ids = [0]# [0, 1, 2, 3]
    torch.cuda.empty_cache()
    gpus_type, gpus_num = torch.cuda.get_device_name(), torch.cuda.device_count()
    print(f"GPU Device: {gpus_type}, GPU Count: {gpus_num}")
    print(f"CUDA Available: {torch.cuda.is_available()}")
    print(f"Current CUDA Device: {torch.cuda.current_device()}")
    
    if config.distributed:
        print('#----------Start DDP----------#')
        dist.init_process_group(backend='nccl', init_method='env://')
        torch.cuda.manual_seed_all(config.seed)
        config.local_rank = torch.distributed.get_rank()





    print('#----------Preparing dataset----------#')
    train_dataset = config.datasets(
        base_dir=config.data_path, list_dir=config.list_dir, split="train",
        transform=transforms.Compose([RandomGenerator([config.input_size_h, config.input_size_w])]))
    
    # åˆ›å»ºslice-by-sliceéªŒè¯æ•°æ®é›†ï¼Œä¸è®­ç»ƒä¿æŒä¸€è‡´
    class ValTransform(object):
        def __init__(self, output_size=[256, 256]):
            self.output_size = output_size
            
        def __call__(self, sample):
            image, label = sample['image'], sample['label']
            
            # ç¡®ä¿æ•°æ®ç±»å‹ä¸€è‡´
            image = image.astype(np.float32)
            label = label.astype(np.float32)
            
            # è°ƒæ•´å¤§å° - ä¸RandomGeneratorä¿æŒä¸€è‡´
            if image.shape != tuple(self.output_size):
                from scipy.ndimage.interpolation import zoom
                x, y = image.shape
                image = zoom(image, (self.output_size[0] / x, self.output_size[1] / y), order=3)
                label = zoom(label, (self.output_size[0] / x, self.output_size[1] / y), order=0)
            
            # ä¼ªHDRä¸‰é€šé“é¢„å¤„ç† - ä¸RandomGeneratorå®Œå…¨ä¸€è‡´
            image_tensor = torch.from_numpy(image)
            
            # åˆ›å»ºä¸‰ä¸ªé€šé“ï¼šåŸå§‹ã€å¢å¼ºå¯¹æ¯”åº¦ã€å¹³æ»‘
            channel1 = image_tensor  # åŸå§‹å›¾åƒ
            channel2 = torch.clamp(image_tensor * 1.2, 0, 1)  # å¢å¼ºå¯¹æ¯”åº¦
            channel3 = F.avg_pool2d(image_tensor.unsqueeze(0).unsqueeze(0), 
                                   kernel_size=3, stride=1, padding=1).squeeze()  # å¹³æ»‘
            
            # åˆå¹¶ä¸ºä¸‰é€šé“
            image = torch.stack([channel1, channel2, channel3], dim=0)
            label = torch.from_numpy(label)
            
            sample = {'image': image, 'label': label.long()}
            return sample
    
    # ä½¿ç”¨slice-by-sliceéªŒè¯æ•°æ®é›†
    val_dataset_slice = config.datasets(
        base_dir=config.data_path,  # ä½¿ç”¨ç›¸åŒçš„æ•°æ®è·¯å¾„
        list_dir=config.list_dir,
        split="test_slice",  # ä½¿ç”¨æµ‹è¯•sliceåˆ—è¡¨
        transform=ValTransform([config.input_size_h, config.input_size_w])
    )
    
    # åŸå§‹volumeéªŒè¯æ•°æ®é›†ï¼ˆä¿ç•™ä½œä¸ºå¯¹æ¯”ï¼‰
    val_dataset = config.datasets(base_dir=config.volume_path, split="test_vol", list_dir=config.list_dir)
    
    print(f"Train dataset size: {len(train_dataset)}")
    print(f"Validation slice dataset size: {len(val_dataset_slice)}")
    print(f"Validation volume dataset size: {len(val_dataset)}")
    
    train_sampler = DistributedSampler(train_dataset) if config.distributed else None
    train_loader = DataLoader(train_dataset, 
                              batch_size=config.batch_size, 
                              num_workers=config.num_workers, 
                              pin_memory=True, 
                              sampler=train_sampler, 
                              shuffle=(train_sampler is None), 
                              worker_init_fn=seed_worker)
    
    # SliceéªŒè¯æ•°æ®åŠ è½½å™¨
    val_loader_slice = DataLoader(val_dataset_slice,
                                 batch_size=config.batch_size,
                                shuffle=False,
                                 num_workers=config.num_workers,
                                 pin_memory=True)
    
    # VolumeéªŒè¯æ•°æ®åŠ è½½å™¨ï¼ˆä¿ç•™ï¼‰
    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=1, pin_memory=True)

    
    


    print('#----------Prepareing Models----------#')
    model_cfg = config.model_config
    
    # æ ¹æ®é…ç½®é€‰æ‹©æ¨¡å‹
    if config.network == 'vmunet':
        model = VMUNet(
            num_classes=model_cfg['num_classes'],
            input_channels=model_cfg['input_channels'],
            depths=model_cfg['depths'],
            depths_decoder=model_cfg['depths_decoder'],
            drop_path_rate=model_cfg['drop_path_rate'],
            load_ckpt_path=model_cfg['load_ckpt_path'],
        )
        if model_cfg['load_ckpt_path'].split('/')[-1] == 'vmamba_small_e238_ema.pth':
            model.load_from()
        else:
            print("å½“å‰æ¨¡å‹çš„çŠ¶æ€å­—å…¸:")
            print(model.state_dict().keys())
            print('---------------------------')

            continue_training = torch.load(model_cfg['load_ckpt_path'])
            print("ç»§ç»­è®­ç»ƒçš„çŠ¶æ€å­—å…¸:")
            print(continue_training.keys())

            # åŠ è½½æ¨¡å‹å‚æ•°
            model_dict = model.state_dict()

            # åªä¿ç•™åœ¨æ¨¡å‹å­—å…¸ä¸­å­˜åœ¨çš„å‚æ•°
            new_dict = {k: v for k, v in continue_training.items() if k in model_dict.keys()}

            # æ›´æ–°æ¨¡å‹çš„çŠ¶æ€å­—å…¸
            model_dict.update(new_dict)

            # æ‰“å°æœªåŠ è½½çš„é”®
            not_loaded = [k for k in model_dict.keys() if k not in new_dict.keys()]
            print(f"æœªåŠ è½½çš„é”®: {not_loaded}")

            # åŠ è½½æ›´æ–°åçš„çŠ¶æ€å­—å…¸
            model.load_state_dict(model_dict)

    elif config.network == 'DWSegNet':
        # åˆ›å»ºargså¯¹è±¡ï¼ŒDWSegNetéœ€è¦è¿™ä¸ªå‚æ•°
        class Args:
            def __init__(self):
                self.dataset = config.datasets_name  # ä½¿ç”¨é…ç½®ä¸­çš„æ•°æ®é›†åç§°
        
        args = Args()
        
        model = DWSegNet(
            args=args,
            img_size=(config.input_size_h, config.input_size_w),
            feature_size=model_cfg.get('feature_size', 48),
            use_boundary_refinement=model_cfg.get('use_boundary_refinement', True)
        )
        
        # å¦‚æœæœ‰é¢„è®­ç»ƒæƒé‡ï¼Œå°è¯•åŠ è½½
        if model_cfg.get('load_ckpt_path') and os.path.exists(model_cfg['load_ckpt_path']):
            try:
                print("å°è¯•åŠ è½½é¢„è®­ç»ƒæƒé‡...")
                checkpoint = torch.load(model_cfg['load_ckpt_path'], map_location='cpu')
                
                # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
                if 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                else:
                    state_dict = checkpoint
                
                # å°è¯•åŠ è½½å…¼å®¹çš„æƒé‡
                model_dict = model.state_dict()
                compatible_dict = {}
                
                for k, v in state_dict.items():
                    if k in model_dict and model_dict[k].shape == v.shape:
                        compatible_dict[k] = v
                
                model_dict.update(compatible_dict)
                model.load_state_dict(model_dict)
                print(f"æˆåŠŸåŠ è½½ {len(compatible_dict)}/{len(model_dict)} ä¸ªæƒé‡å‚æ•°")
                
            except Exception as e:
                print(f"é¢„è®­ç»ƒæƒé‡åŠ è½½å¤±è´¥: {e}")
                print("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
    
    elif config.network == 'HBFormer':
        # åˆ›å»ºargså¯¹è±¡ï¼ŒAFFSegNetéœ€è¦è¿™ä¸ªå‚æ•°
        class Args:
            def __init__(self):
                self.dataset = config.datasets_name  # ä½¿ç”¨é…ç½®ä¸­çš„æ•°æ®é›†åç§°
        
        args = Args()
        
        model = HBFormer(
            args=args,
            img_size=(config.input_size_h, config.input_size_w),
            feature_size=model_cfg.get('feature_size', 48)
        )
        
        # å¦‚æœæœ‰é¢„è®­ç»ƒæƒé‡ï¼Œå°è¯•åŠ è½½
        if model_cfg.get('load_ckpt_path') and os.path.exists(model_cfg['load_ckpt_path']):
            try:
                print("å°è¯•åŠ è½½é¢„è®­ç»ƒæƒé‡...")
                checkpoint = torch.load(model_cfg['load_ckpt_path'], map_location='cpu')
                
                # å¤„ç†ä¸åŒçš„checkpointæ ¼å¼
                if 'model_state_dict' in checkpoint:
                    state_dict = checkpoint['model_state_dict']
                elif 'model' in checkpoint:
                    state_dict = checkpoint['model']
                else:
                    state_dict = checkpoint
                
                # å°è¯•åŠ è½½å…¼å®¹çš„æƒé‡
                model_dict = model.state_dict()
                compatible_dict = {}
                
                for k, v in state_dict.items():
                    if k in model_dict and model_dict[k].shape == v.shape:
                        compatible_dict[k] = v
                
                model_dict.update(compatible_dict)
                model.load_state_dict(model_dict)
                print(f"æˆåŠŸåŠ è½½ {len(compatible_dict)}/{len(model_dict)} ä¸ªæƒé‡å‚æ•°")
                
            except Exception as e:
                print(f"é¢„è®­ç»ƒæƒé‡åŠ è½½å¤±è´¥: {e}")
                print("å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")

    elif config.network == 'SMAFormer':
        # åˆ›å»ºargså¯¹è±¡ï¼ŒSMAFormeréœ€è¦è¿™ä¸ªå‚æ•°
        class Args:
            def __init__(self):
                self.dataset = config.datasets_name  # ä½¿ç”¨é…ç½®ä¸­çš„æ•°æ®é›†åç§°
        
        args = Args()
        
        # 2025-12-21æ”¹è¿›ç‰ˆSMAFormer (ViT-Base)
        # é‡‡ç”¨Adapteré£æ ¼SMA + æ··åˆä½ç½®ç¼–ç  + DPTå¤šå°ºåº¦ç‰¹å¾ + UNetè§£ç å™¨
        model = SMAFormer(
            args=args,
            img_size=config.input_size_h,
            in_chans=model_cfg.get('input_channels', 3),
            embed_dim=model_cfg.get('embed_dim', 768),  # ViT-Baseå›ºå®š768
            depth=model_cfg.get('depth', 12),  # ViT-Baseå›ºå®š12ä¸ªblocks
            num_heads=model_cfg.get('num_heads', 12),  # ViT-Baseå›ºå®š12ä¸ªheads
            mlp_ratio=model_cfg.get('mlp_ratio', 4.),
            drop_rate=model_cfg.get('drop_rate', 0.),
            attn_drop_rate=model_cfg.get('attn_drop_rate', 0.),
            drop_path_rate=model_cfg.get('drop_path_rate', 0.1),
            pretrained=model_cfg.get('pretrained', True),
            pretrained_path=model_cfg.get('pretrained_path', 'pre_trained_weights'),
            use_sma=model_cfg.get('use_sma', True),  # ä½¿ç”¨SMA Adapter
        )
        print(f"SMAFormeræ¨¡å‹åˆ›å»ºæˆåŠŸ (æ”¹è¿›ç‰ˆ, Adapteré£æ ¼SMA), è¾“å‡ºç±»åˆ«æ•°: {model.num_classes}")

    elif config.network == 'SMAFormerV2':
        # åˆ›å»ºargså¯¹è±¡
        class Args:
            def __init__(self):
                self.dataset = config.datasets_name
        
        args = Args()
        
        # è·å–å›¾åƒå°ºå¯¸ï¼Œæ”¯æŒintæˆ–tupleæ ¼å¼
        img_size_cfg = model_cfg.get('img_size', config.input_size_h)
        if isinstance(img_size_cfg, (list, tuple)):
            img_size = img_size_cfg[0]  # å–ç¬¬ä¸€ä¸ªç»´åº¦
        else:
            img_size = img_size_cfg
        
        # V2.3æ”¹è¿›ç‰ˆSMAFormerV2 (Swin-Tinyé•œåƒEncoder-Decoder)
        # åŒå‘é¢„è®­ç»ƒæƒé‡åŠ è½½ï¼šEncoder 100%, Decoder ~86%
        model = SMAFormerV2(
            args=args,
            img_size=img_size,
            num_classes=model_cfg.get('num_classes', 9),
            embed_dims=model_cfg.get('embed_dims', [96, 192, 384, 768]),
            depths=model_cfg.get('depths', [2, 2, 6, 2]),
            num_heads=model_cfg.get('num_heads', [3, 6, 12, 24]),
            window_size=model_cfg.get('window_size', 7),
            mlp_ratio=model_cfg.get('mlp_ratio', 4.),
            drop_rate=model_cfg.get('drop_rate', 0.),
            drop_path_rate=model_cfg.get('drop_path_rate', 0.2),
            use_sma=model_cfg.get('use_sma', True),
            pretrained_path=model_cfg.get('pretrained_path', 'pre_trained_weights/swin_tiny_patch4_window7_224.pth'),
            load_pretrained=model_cfg.get('load_pretrained', True)
        )
        print(f"SMAFormerV2 V2.3æ¨¡å‹åˆ›å»ºæˆåŠŸ (é•œåƒEncoder-Decoder, åŒå‘é¢„è®­ç»ƒ), è¾“å‡ºç±»åˆ«æ•°: {model.num_classes}")

    else:
        raise ValueError(f'ä¸æ”¯æŒçš„ç½‘ç»œç±»å‹: {config.network}ã€‚è¯·é€‰æ‹©: vmunet, DWSegNet, HBFormer, SMAFormer, SMAFormerV2')

    model = model.cuda()
    cal_params_flops(model, config.input_size_w, logger)

    if config.distributed:
        model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model).cuda()
        model = DDP(model, device_ids=[config.local_rank], output_device=config.local_rank)
    else:
        model = torch.nn.DataParallel(model.cuda(), device_ids=gpu_ids, output_device=gpu_ids[0])

    # æ‰“å°è®­ç»ƒé…ç½®æ€»ç»“
    print('\n' + '='*80)
    logger.info('='*80)
    print('Training Configuration Summary')
    logger.info('Training Configuration Summary')
    print('='*80)
    logger.info('='*80)
    print(f'ğŸ“¦ Model: {config.network}')
    logger.info(f'Model: {config.network}')
    print(f'ğŸ“Š Dataset: {config.datasets_name}')
    logger.info(f'Dataset: {config.datasets_name}')
    print(f'ğŸ–¼ï¸  Image Size: {config.input_size_h}x{config.input_size_w}')
    logger.info(f'Image Size: {config.input_size_h}x{config.input_size_w}')
    print(f'ğŸ¯ Classes: {config.num_classes}')
    logger.info(f'Classes: {config.num_classes}')
    print(f'ğŸ“ˆ Epochs: {config.epochs}')
    logger.info(f'Epochs: {config.epochs}')
    print(f'ğŸ”¢ Batch Size: {config.batch_size}')
    logger.info(f'Batch Size: {config.batch_size}')
    print(f'âš™ï¸  Optimizer: {config.opt}, LR: {config.lr}')
    logger.info(f'Optimizer: {config.opt}, LR: {config.lr}')
    print(f'ğŸ“… Scheduler: {config.sch}')
    logger.info(f'Scheduler: {config.sch}')
    
    # å¦‚æœæ˜¯SMAFormerï¼Œæ‰“å°ç‰¹æ®Šé…ç½®
    if config.network == 'SMAFormer':
        print('\n--- SMAFormer Enhancement Features ---')
        logger.info('--- SMAFormer Enhancement Features ---')
        sma_mode = model_cfg.get('sma_mode', 'parallel')
        use_multi = model_cfg.get('use_multi_scale', False)
        use_enhanced = model_cfg.get('use_enhanced_decoder', False)
        
        print(f'âœ¨ SMA Mode: {sma_mode} (æ–¹æ¡ˆA/D: {"å¼€å¯" if sma_mode == "parallel" else "ç¦ç”¨"})')
        logger.info(f'SMA Mode: {sma_mode} (æ–¹æ¡ˆA/D: {"å¼€å¯" if sma_mode == "parallel" else "ç¦ç”¨"})')
        print(f'âœ¨ Multi-Scale Features: {use_multi} (æ–¹æ¡ˆB: {"å¼€å¯" if use_multi else "ç¦ç”¨"})')
        logger.info(f'Multi-Scale Features: {use_multi} (æ–¹æ¡ˆB: {"å¼€å¯" if use_multi else "ç¦ç”¨"})')
        print(f'âœ¨ Enhanced Decoder: {use_enhanced} (æ–¹æ¡ˆC: {"å¼€å¯" if use_enhanced else "ç¦ç”¨"})')
        logger.info(f'Enhanced Decoder: {use_enhanced} (æ–¹æ¡ˆC: {"å¼€å¯" if use_enhanced else "ç¦ç”¨"})')
        
        # ç»™å‡ºé…ç½®å»ºè®®
        if sma_mode == 'parallel' and use_multi and use_enhanced:
            print('ğŸ’¡ Configuration: å®Œæ•´ç‰ˆ (æ¨è) - æ‰€æœ‰æ”¹è¿›æ–¹æ¡ˆå‡å·²å¼€å¯')
            logger.info('Configuration: å®Œæ•´ç‰ˆ (æ¨è) - æ‰€æœ‰æ”¹è¿›æ–¹æ¡ˆå‡å·²å¼€å¯')
        elif sma_mode == 'disabled' and not use_multi and not use_enhanced:
            print('ğŸ’¡ Configuration: Baseline - ä»…ä½¿ç”¨åŸºç¡€ViTæ¶æ„')
            logger.info('Configuration: Baseline - ä»…ä½¿ç”¨åŸºç¡€ViTæ¶æ„')
        else:
            print('ğŸ’¡ Configuration: éƒ¨åˆ†åŠŸèƒ½ç‰ˆ - éƒ¨åˆ†æ”¹è¿›æ–¹æ¡ˆå¼€å¯')
            logger.info('Configuration: éƒ¨åˆ†åŠŸèƒ½ç‰ˆ - éƒ¨åˆ†æ”¹è¿›æ–¹æ¡ˆå¼€å¯')
    
    print('='*80)
    logger.info('='*80)
    print('\n')



    print('#----------Prepareing loss, opt, sch and amp----------#')
    criterion = config.criterion
    optimizer = get_optimizer(config, model)
    scheduler = get_scheduler(config, optimizer)
    scaler = GradScaler()





    print('#----------Set other params----------#')
    min_loss = 999
    start_epoch = 1
    min_epoch = 1

    # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦æ¢å¤è®­ç»ƒ
    if config.resume_training and os.path.exists(resume_model):
        print('#----------Resume Model and Other params----------#')
        checkpoint = torch.load(resume_model, map_location=torch.device('cpu'))
        model.module.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        saved_epoch = checkpoint['epoch']
        start_epoch = saved_epoch + 1  # ä¿®å¤ï¼šä»ä¸‹ä¸€ä¸ªepochå¼€å§‹ï¼Œè€Œä¸æ˜¯ç´¯åŠ 
        min_loss, min_epoch, loss = checkpoint['min_loss'], checkpoint['min_epoch'], checkpoint['loss']
        best_dice = checkpoint.get('best_dice', 0.0)  # æ¢å¤best_diceï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è®¾ä¸º0

        log_info = f'resuming model from {resume_model}. resume_epoch: {saved_epoch}, min_loss: {min_loss:.4f}, min_epoch: {min_epoch}, loss: {loss:.4f}, best_dice: {best_dice:.4f}'
        logger.info(log_info)
    else:
        if not config.resume_training:
            print('#----------Starting fresh training (resume_training=False)----------#')
        else:
            print('#----------No checkpoint found, starting fresh training----------#')





    print('#----------Training----------#')
    best_dice = 0.0
    patience = 100  # æ—©åœè€å¿ƒ       
    patience_counter = 0  # è®¡æ•°å™¨
    
    for epoch in range(start_epoch, config.epochs + 1):

        torch.cuda.empty_cache()
        train_sampler.set_epoch(epoch) if config.distributed else None

        # è®°å½•å½“å‰epochå¼€å§‹æ—¶çš„å­¦ä¹ ç‡ï¼ˆåœ¨scheduler.step()ä¹‹å‰ï¼‰
        current_lr = optimizer.state_dict()['param_groups'][0]['lr']

        train_metrics = train_one_epoch(
            train_loader,
            model,
            criterion,
            optimizer,
            scheduler,
            epoch,
            logger,
            config,
            scaler=scaler
        )

        if train_metrics['loss'] < min_loss:
            torch.save(model.module.state_dict(), os.path.join(checkpoint_dir, 'best.pth'))
            min_loss = train_metrics['loss']
            min_epoch = epoch

        # Initialize validation metrics
        val_metrics = None

        if epoch % config.val_interval == 0:
            # ä½¿ç”¨slice-by-sliceéªŒè¯ï¼ˆæ¨èï¼‰
            from engine_synapse import val_one_epoch_slice
            val_metrics = val_one_epoch_slice(
                val_dataset_slice,
                val_loader_slice,
                model,
                epoch,
                logger,
                config,
                test_save_path=outputs,
                val_or_test=False
            )
            
            # å¯é€‰ï¼šåŒæ—¶è¿›è¡ŒvolumeéªŒè¯ä½œä¸ºå¯¹æ¯”
            # if epoch % (config.val_interval * 3) == 0:  # æ¯3ä¸ªéªŒè¯å‘¨æœŸè¿›è¡Œä¸€æ¬¡volumeéªŒè¯
            #     logger.info("è¿›è¡ŒvolumeéªŒè¯ä½œä¸ºå¯¹æ¯”...")
            #     val_metrics_volume = val_one_epoch(
            #         val_dataset,
            #         val_loader,
            #         model,
            #         epoch,
            #         logger,
            #         config,
            #         test_save_path=outputs,
            #         val_or_test=False
            #     )
            #     logger.info(f"VolumeéªŒè¯ç»“æœ: avg_dice={val_metrics_volume['avg_dice']:.4f}, sliceéªŒè¯ç»“æœ: avg_dice={val_metrics['avg_dice']:.4f}")
            
            # Save best model based on dice score
            if val_metrics['avg_dice'] > best_dice:
                best_dice = val_metrics['avg_dice']
                torch.save(model.module.state_dict(), os.path.join(checkpoint_dir, 'best_dice.pth'))
                patience_counter = 0  # é‡ç½®è®¡æ•°å™¨
            else:
                patience_counter += 1  # å¢åŠ è®¡æ•°å™¨

            # æ‰“å°æ—©åœä¿¡æ¯
            logger.info(f'Best validation dice: {best_dice:.4f}, current: {val_metrics["avg_dice"]:.4f}, patience: {patience_counter}/{patience}')
        
        # Record training metrics to CSV
        train_record = {
            'epoch': epoch,
            'lr': current_lr,
            'loss': train_metrics['loss'],
            'avg_dice': train_metrics['avg_dice'],
            'miou': train_metrics['miou']
        }
        
        # Add per-organ dice scores for training
        for i, organ in enumerate(organ_names):
            train_record[f'dice_{organ}'] = train_metrics['dice_per_class'][i]
            
        with open(train_csv_file, 'a', newline='') as f:
            writer = csv.DictWriter(f, fieldnames=train_columns)
            writer.writerow(train_record)
        
        # Record validation metrics to CSV if validation was performed
        if val_metrics is not None:
            val_record = {
                'epoch': epoch,
                'avg_dice': val_metrics['avg_dice'],
                'avg_hd95': val_metrics['avg_hd95'],
                'miou': val_metrics['miou']
            }
            
            # Add per-organ dice and hd95 scores for validation
            for i, organ in enumerate(organ_names):
                val_record[f'dice_{organ}'] = val_metrics['dice_per_class'][i]
                val_record[f'hd95_{organ}'] = val_metrics['hd95_per_class'][i]
                
            with open(val_csv_file, 'a', newline='') as f:
                writer = csv.DictWriter(f, fieldnames=val_columns)
                writer.writerow(val_record)

        torch.save(
            {
                'epoch': epoch,
                'min_loss': min_loss,
                'min_epoch': min_epoch,
                'loss': train_metrics['loss'],
                'best_dice': best_dice,
                'model_state_dict': model.module.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
            }, os.path.join(checkpoint_dir, 'latest.pth')) 

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ—©åœ
        if val_metrics is not None and patience_counter >= patience:
            print(f'Early stopping at epoch {epoch} (no improvement for {patience} validation epochs)')
            logger.info(f'Early stopping at epoch {epoch} (no improvement for {patience} validation epochs)')
            break

        # å®šæœŸä¿å­˜æ¨¡å‹æƒé‡
        if epoch % config.save_interval == 0:
            save_path = os.path.join(checkpoint_dir, f'epoch_{epoch}.pth')
            torch.save(model.module.state_dict(), save_path)
            logger.info(f'å®šæœŸä¿å­˜æ¨¡å‹æƒé‡: {save_path}')

    print('#----------Training Completed----------#')
    print(f'Best loss: {min_loss:.4f} at epoch {min_epoch}')
    print(f'Best dice: {best_dice:.4f}')
    print('Use test_synapse.py for final evaluation')


if __name__ == '__main__':
    config = setting_config()
    main(config)       