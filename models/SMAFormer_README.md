# SMAFormer: Spatial-Mamba Attention Transformer for Medical Image Segmentation

**IEEE BIBM 2024**

## æ¦‚è¿°

SMAFormeræ˜¯ä¸€ä¸ªåˆ›æ–°çš„åŒ»å­¦å›¾åƒåˆ†å‰²æ¶æ„ï¼Œå°†Vision Transformer (ViT)çš„å…¨å±€å»ºæ¨¡èƒ½åŠ›ä¸Spatial-Mamba Attention (SMA)çš„é«˜æ•ˆç©ºé—´æ„ŸçŸ¥æœºåˆ¶ç›¸ç»“åˆã€‚è¯¥æ¶æ„ä¸“ä¸ºå¤šå™¨å®˜åŒ»å­¦å›¾åƒåˆ†å‰²è®¾è®¡ï¼Œåœ¨Synapseå¤šå™¨å®˜åˆ†å‰²æ•°æ®é›†ä¸Šå–å¾—äº†ä¼˜å¼‚æ€§èƒ½ã€‚

## æ ¸å¿ƒåˆ›æ–°

### 1. Spatial-Mamba Attention (SMA)
SMAæ¨¡å—æ•´åˆäº†ä¸¤ä¸ªå…³é”®ç»„ä»¶ï¼š
- **Pixel-wise Spatial Attention**: æ•è·ç©ºé—´ç»´åº¦çš„å±€éƒ¨ç‰¹å¾å’Œä½ç½®ä¿¡æ¯
- **Channel-wise Mamba**: å»ºæ¨¡é€šé“é—´çš„é•¿ç¨‹ä¾èµ–å…³ç³»

SMAé€šè¿‡é«˜æ•ˆçš„é—¨æ§èåˆæœºåˆ¶å¹³è¡¡ç©ºé—´å’Œé€šé“ä¿¡æ¯ï¼Œå®ç°äº†ä¼˜äºä¼ ç»Ÿæ³¨æ„åŠ›çš„æ€§èƒ½ã€‚

### 2. å¹¶è¡ŒåŒè·¯å¾„æ¶æ„
ä¸åŒäºä¼ ç»Ÿçš„ä¸²è¡Œåå¤„ç†æ–¹å¼ï¼ŒSMAFormeré‡‡ç”¨å¹¶è¡ŒåŒè·¯å¾„è®¾è®¡ï¼š

```
Input Features
    â”œâ”€â”€â†’ Self-Attention Branch  â†’  Global Context
    â”‚                              â†“
    â””â”€â”€â†’ SMA Branch            â†’  Spatial-Channel Awareness
                                   â†“
                        Gated Fusion (Î±Â·SA + Î²Â·SMA)
                                   â†“
                              Output Features
```

è¿™ç§è®¾è®¡å…è®¸æ¨¡å‹åŒæ—¶å¤„ç†å…¨å±€ä¸Šä¸‹æ–‡å’Œç©ºé—´ç»†èŠ‚ï¼Œå¹¶é€šè¿‡è‡ªé€‚åº”é—¨æ§æœºåˆ¶åŠ¨æ€èåˆã€‚

### 3. å¤šå°ºåº¦ç‰¹å¾æå– (DPT-Style)
å€Ÿé‰´DPT (Dense Prediction Transformer)çš„æ€æƒ³ï¼Œä»ViTçš„ä¸­é—´å±‚æå–å¤šå°ºåº¦ç‰¹å¾ï¼š

```
ViT Blocks      Feature      Reassemble       Output Scale
-----------     -------      ----------       ------------
Blocks 0-2   â†’  F1(768D)  â†’  UpsampleÃ—2   â†’  C1: 96Ã—64Ã—64
Blocks 3-5   â†’  F2(768D)  â†’  Keep         â†’  C2: 192Ã—32Ã—32
Blocks 6-8   â†’  F3(768D)  â†’  DownsampleÃ—2 â†’  C3: 384Ã—16Ã—16
Blocks 9-11  â†’  F4(768D)  â†’  DownsampleÃ—4 â†’  C4: 768Ã—8Ã—8
```

è¿™ç§åˆ†å±‚ç‰¹å¾é‡‘å­—å¡”ä¸ºåç»­è§£ç å™¨æä¾›äº†ä¸°å¯Œçš„å¤šå°ºåº¦ä¿¡æ¯ã€‚

### 4. å¢å¼ºå‹Decoder
é›†æˆå¤šç§å…ˆè¿›æŠ€æœ¯çš„è§£ç å™¨è®¾è®¡ï¼š

- **ASPP (Atrous Spatial Pyramid Pooling)**: å¤šå°ºåº¦ä¸Šä¸‹æ–‡èšåˆ
- **SE (Squeeze-and-Excitation) Attention**: é€šé“æ³¨æ„åŠ›å¢å¼º
- **UNet-style Skip Connections**: è·¨å±‚ç‰¹å¾èåˆ

è§£ç å™¨é€æ­¥ä¸Šé‡‡æ ·å¹¶èåˆå¤šå°ºåº¦ç‰¹å¾ï¼Œæœ€ç»ˆæ¢å¤åˆ°åŸå§‹åˆ†è¾¨ç‡çš„åˆ†å‰²æ©ç ã€‚

## æ¶æ„è®¾è®¡

### æ•´ä½“æµç¨‹

```
Input Image (HÃ—WÃ—3)
        â†“
    Patch Embedding (16Ã—16 patches)
        â†“
    Position Embedding
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Multi-Scale ViT Encoder      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Parallel SMA-Transformer â”‚  â”‚
â”‚  â”‚ Blocks (Ã—12)            â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚  â”‚
â”‚  â”‚  â”‚ Layer Norm       â”‚   â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚  â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ â”‚  SA  â”‚  SMA   â”‚â”‚   â”‚  â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”¬â”€â”€â”€â”€â”˜â”‚   â”‚  â”‚
â”‚  â”‚  â”‚    â””â”€Gateâ”€â”€â”˜     â”‚   â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚  â”‚
â”‚  â”‚  â”‚ Layer Norm       â”‚   â”‚  â”‚
â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚  â”‚
â”‚  â”‚  â”‚ MLP (FFN)        â”‚   â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚    â†“      â†“      â†“      â†“     â”‚
â”‚   C1     C2     C3     C4     â”‚
â”‚ 64Ã—64  32Ã—32  16Ã—16   8Ã—8     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Enhanced Decoder             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ ASPP on C4              â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ Decoder Block 3         â”‚  â”‚
â”‚  â”‚  â”œâ”€ Skip from C3        â”‚  â”‚
â”‚  â”‚  â””â”€ SE Attention        â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ Decoder Block 2         â”‚  â”‚
â”‚  â”‚  â”œâ”€ Skip from C2        â”‚  â”‚
â”‚  â”‚  â””â”€ SE Attention        â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ Decoder Block 1         â”‚  â”‚
â”‚  â”‚  â”œâ”€ Skip from C1        â”‚  â”‚
â”‚  â”‚  â””â”€ SE Attention        â”‚  â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚
â”‚  â”‚ Progressive Upsample    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Output Segmentation (HÃ—WÃ—9)
```

### SMAæ¨¡å—è¯¦ç»†è®¾è®¡

```python
# Improved SMA Module with Matrix Fusion
class ImprovedSMAModule(nn.Module):
    """
    ç©ºé—´-é€šé“æ··åˆæ³¨æ„åŠ›æ¨¡å—
    
    å·¥ä½œæµç¨‹:
    1. Pixel-wise Spatial: Conv â†’ ç©ºé—´æ³¨æ„åŠ›æƒé‡
    2. Channel-wise Mamba: Linear â†’ Mamba â†’ é€šé“å»ºæ¨¡
    3. Gated Fusion: é—¨æ§æœºåˆ¶åŠ¨æ€èåˆä¸¤è·¯è¾“å‡º
    """
    def forward(self, x):
        # x: [B, N, C]
        pixel_out = self.spatial_branch(x)    # ç©ºé—´å¤„ç†
        channel_out = self.channel_branch(x)   # é€šé“å¤„ç†
        gate = self.gate(torch.cat([pixel_out, channel_out], dim=-1))
        return gate * pixel_out + (1 - gate) * channel_out
```

### å¹¶è¡ŒSMA-Transformer Block

```python
class ImprovedSMATransformerBlock(nn.Module):
    """
    å¹¶è¡Œå¤„ç†SAå’ŒSMAï¼Œé—¨æ§èåˆ
    
    å‰å‘è¿‡ç¨‹:
    1. æ ‡å‡†åŒ–è¾“å…¥
    2. å¹¶è¡Œè®¡ç®—:
       - Self-Attentionåˆ†æ”¯
       - SMAåˆ†æ”¯
    3. é—¨æ§èåˆä¸¤è·¯è¾“å‡º
    4. æ®‹å·®è¿æ¥
    5. FFNå¤„ç†
    """
    def forward(self, x, H, W):
        # Norm + Parallel Dual-Path
        x_norm = self.norm1(x)
        attn_out = self.attn(x_norm)
        sma_out = self.sma(x_norm, H, W)
        
        # Gated Fusion
        gate = self.gate_mlp(torch.cat([attn_out, sma_out], dim=-1))
        fused = gate * attn_out + (1 - gate) * sma_out
        
        # Residual + FFN
        x = x + self.drop_path(fused)
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x
```

## æ¨¡å‹é…ç½®

### ViT-Base Backbone
- **Patch Size**: 16Ã—16
- **Embedding Dimension**: 768
- **Depth**: 12 Transformer blocks
- **Heads**: 12 attention heads
- **MLP Ratio**: 4.0
- **Pretrained**: ImageNet-21K ViT-Base

### Multi-Scale Encoder
- **Extract Layers**: [2, 5, 8, 11] (after blocks 2, 5, 8, 11)
- **Output Channels**: [96, 192, 384, 768]
- **Scale Factors**: [2.0, 1.0, 0.5, 0.25]

### Enhanced Decoder
- **ASPP**: dilation rates [1, 6, 12, 18]
- **Decoder Stages**: 3 stages (256â†’128â†’64 channels)
- **SE Reduction**: 16
- **Final Upsampling**: 4Ã—

## æ€§èƒ½ç‰¹ç‚¹

### å‚æ•°é‡ä¸è®¡ç®—é‡
- **Total Parameters**: 138.89M
- **Pretrained Parameters**: 85.80M (61.8%)
- **FLOPs**: ~125 GFLOPs (at 256Ã—256 input)

### è®­ç»ƒé…ç½®
- **Input Size**: 256Ã—256
- **Batch Size**: 28
- **Optimizer**: AdamW (lr=0.0001)
- **Loss Function**: CE + Dice
- **Data Augmentation**: Pseudo-HDR preprocessing

### æ¨ç†é€Ÿåº¦
- **RTX 4090**: ~30ms per image (256Ã—256)
- **Memory**: ~2GB GPU memory for inference

## ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ä½¿ç”¨

```python
from models.SMAFormer import SMAFormer

# åˆ›å»ºæ¨¡å‹å®ä¾‹
class Args:
    def __init__(self):
        self.dataset = 'Synapse'

args = Args()

model = SMAFormer(
    args=args,
    img_size=256,                      # è¾“å…¥å›¾åƒå°ºå¯¸
    in_chans=3,                        # è¾“å…¥é€šé“æ•°
    num_classes=9,                     # è¾“å‡ºç±»åˆ«æ•°
    embed_dim=768,                     # ViT-Baseå›ºå®š
    depth=12,                          # ViT-Baseå›ºå®š
    num_heads=12,                      # ViT-Baseå›ºå®š
    pretrained=True,                   # ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
    pretrained_path='pre_trained_weights',
    
    # æ¶æ„é…ç½®
    use_multi_scale=True,              # å¯ç”¨å¤šå°ºåº¦ç‰¹å¾
    use_enhanced_decoder=True,         # å¯ç”¨å¢å¼ºDecoder
    sma_mode='parallel',               # å¹¶è¡ŒSMAæ¨¡å¼
)

# å‰å‘ä¼ æ’­
import torch
x = torch.randn(1, 3, 256, 256).cuda()
output = model(x)  # [1, 9, 256, 256]
```

### é…ç½®é€‰é¡¹

```python
# æ–¹æ¡ˆA: ä»…å¹¶è¡ŒSMA
model = SMAFormer(
    args=args,
    img_size=256,
    sma_mode='parallel',           # å¯ç”¨å¹¶è¡ŒSMA
    use_multi_scale=False,
    use_enhanced_decoder=False
)

# æ–¹æ¡ˆB: å¤šå°ºåº¦ç‰¹å¾
model = SMAFormer(
    args=args,
    img_size=256,
    sma_mode='disabled',           # ç¦ç”¨SMA
    use_multi_scale=True,          # å¤šå°ºåº¦ç‰¹å¾
    use_enhanced_decoder=False
)

# æ–¹æ¡ˆC: å¢å¼ºDecoder
model = SMAFormer(
    args=args,
    img_size=256,
    sma_mode='disabled',
    use_multi_scale=False,
    use_enhanced_decoder=True      # å¢å¼ºDecoder
)

# å®Œæ•´ç‰ˆ (æ¨è): A+B+C
model = SMAFormer(
    args=args,
    img_size=256,
    sma_mode='parallel',           # æ–¹æ¡ˆA
    use_multi_scale=True,          # æ–¹æ¡ˆB
    use_enhanced_decoder=True      # æ–¹æ¡ˆC
)
```

## é¢„è®­ç»ƒæƒé‡

### ä¸‹è½½æƒé‡
ViT-Baseé¢„è®­ç»ƒæƒé‡ä¼šè‡ªåŠ¨ä¸‹è½½åˆ°`pre_trained_weights/`ç›®å½•ï¼š
```
pre_trained_weights/
â””â”€â”€ jx_vit_base_patch16_224-8ee2ff3e.pth
```

### æƒé‡åŠ è½½ç»Ÿè®¡
æ¨¡å‹åˆå§‹åŒ–æ—¶ä¼šæ˜¾ç¤ºè¯¦ç»†çš„æƒé‡åŠ è½½ä¿¡æ¯ï¼š
```
============================================================
âœ… SMAFormeré¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸï¼
============================================================
ğŸ“Š åŠ è½½ç»Ÿè®¡:
   - æˆåŠŸåŠ è½½çš„å‚æ•°å±‚æ•°: 149
   - æˆåŠŸåŠ è½½çš„å‚æ•°é‡: 85,797,120
   - ViT-Baseæœ‰æ•ˆå‚æ•°é‡: 85,797,120
   - ğŸ¯ é¢„è®­ç»ƒæƒé‡åŠ è½½ç‡: 100.00%
   - æ¨¡å‹å‚æ•°è¦†ç›–ç‡: 61.8%
   - éšæœºåˆå§‹åŒ–å±‚æ•°: 660 (decoderéƒ¨åˆ†)
============================================================
```

## è®­ç»ƒ

### æ•°æ®å‡†å¤‡
è¯·å‚è€ƒé¡¹ç›®æ ¹ç›®å½•çš„æ•°æ®å‡†å¤‡æ–‡æ¡£ã€‚Synapseæ•°æ®é›†åº”ç»„ç»‡ä¸ºï¼š
```
data/Synapse/
â”œâ”€â”€ train_npz/
â”‚   â”œâ”€â”€ case0001_slice000.npz
â”‚   â”œâ”€â”€ case0001_slice001.npz
â”‚   â””â”€â”€ ...
â””â”€â”€ test_vol/
    â”œâ”€â”€ case0001.npy.h5
    â”œâ”€â”€ case0002.npy.h5
    â””â”€â”€ ...
```

### å¼€å§‹è®­ç»ƒ

```bash
# æ¿€æ´»ç¯å¢ƒ
conda activate seg

# è®­ç»ƒSMAFormer (å®Œæ•´ç‰ˆ)
python train_synapse.py --max_epochs 150

# è®­ç»ƒç‰¹å®šé…ç½®
# ä¿®æ”¹ configs/config_setting_synapse.py ä¸­çš„å‚æ•°
```

### é…ç½®æ–‡ä»¶

ä¿®æ”¹`configs/config_setting_synapse.py`:

```python
smaformer_config = {
    'num_classes': 9,
    'input_channels': 3,
    'embed_dim': 768,
    'depth': 12,
    'num_heads': 12,
    'pretrained': True,
    
    # æ¶æ„å¼€å…³
    'sma_mode': 'parallel',              # 'parallel', 'disabled', 'original'
    'use_multi_scale': True,             # å¤šå°ºåº¦ç‰¹å¾
    'use_enhanced_decoder': True,        # å¢å¼ºDecoder
}
```

## è¯„ä¼°

```bash
# æµ‹è¯•æ¨¡å‹
python test_synapse.py --model_path checkpoints/best_model.pth
```

## æŠ€æœ¯ç»†èŠ‚

### ä¼ªHDRé¢„å¤„ç†
è¾“å…¥å›¾åƒè¢«è½¬æ¢ä¸ºä¸‰é€šé“ä¼ªHDRè¡¨ç¤ºï¼š
- **Channel 1**: åŸå§‹å›¾åƒ
- **Channel 2**: å¢å¼ºå¯¹æ¯”åº¦ (Ã—1.2)
- **Channel 3**: å¹³æ»‘ç‰ˆæœ¬ (avg_pool 3Ã—3)

### ä½ç½®ç¼–ç æ’å€¼
ViTé¢„è®­ç»ƒæƒé‡ä½¿ç”¨224Ã—224è¾“å…¥(14Ã—14 patch grid)ï¼Œè®­ç»ƒæ—¶è‡ªåŠ¨æ’å€¼åˆ°256Ã—256(16Ã—16 grid)ï¼š
```python
# è‡ªåŠ¨æ’å€¼ä½ç½®ç¼–ç 
pos_embed_old = [1, 197, 768]  # 1 cls + 196 patches
pos_embed_new = [1, 257, 768]  # 1 cls + 256 patches
# é€šè¿‡åŒçº¿æ€§æ’å€¼å®ç°
```

### æŸå¤±å‡½æ•°
æ··åˆæŸå¤±å‡½æ•°ç»“åˆCEå’ŒDiceï¼š
```python
Loss = CrossEntropy + DiceLoss
```

## æ¶ˆèç ”ç©¶

ä¸åŒé…ç½®çš„å‚æ•°é‡å¯¹æ¯”ï¼š

| é…ç½® | å‚æ•°é‡ | è¯´æ˜ |
|------|--------|------|
| åŸºçº¿ (ViT-only) | 89.77M | ç¦ç”¨SMA |
| +Original SMA | 104.85M | ä¸²è¡ŒSMA |
| +Parallel SMA (A) | 126.12M | å¹¶è¡Œé—¨æ§SMA |
| +Multi-Scale (B) | 97.51M | å¤šå°ºåº¦ç‰¹å¾ |
| +Enhanced Decoder (C) | 103.13M | å¢å¼ºè§£ç å™¨ |
| **å®Œæ•´ç‰ˆ (A+B+C)** | **138.89M** | **æ‰€æœ‰æ”¹è¿›** |

## ä¾èµ–é¡¹

```
torch>=1.10.0
timm>=0.6.0
einops>=0.6.0
numpy>=1.21.0
scipy>=1.7.0
```

## å¼•ç”¨

å¦‚æœæ‚¨ä½¿ç”¨äº†SMAFormer,è¯·å¼•ç”¨ï¼š

```bibtex
@inproceedings{smaformer2024,
  title={SMAFormer: Spatial-Mamba Attention Transformer for Medical Image Segmentation},
  author={Your Name},
  booktitle={IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
  year={2024}
}
```

## è‡´è°¢

æœ¬å·¥ä½œåŸºäºä»¥ä¸‹ä¼˜ç§€é¡¹ç›®ï¼š
- Vision Transformer (ViT) - Google Research
- Mamba - State Space Models
- DPT - Dense Prediction Transformer
- VMUNet - Vision Mamba UNet

## è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚

## è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æIssueæˆ–Pull Requestã€‚

---

**Last Updated**: December 2024
